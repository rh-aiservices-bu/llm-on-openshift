{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a331fd6",
   "metadata": {},
   "source": [
    "## RAG example with Langchain, Redis, and HFTGI\n",
    "\n",
    "Requirements:\n",
    "- A Redis cluster and Database where documents have been injected\n",
    "- All information for connecting to the redis cluster and database, index name and schema file.\n",
    "- An inference endpoint served with Hugging Face Text Generation Inference server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd4537b",
   "metadata": {},
   "source": [
    "#### Bases parameters, Inference server and Redis info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51baf1a6-4111-4b40-b43a-833438bdc222",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inference_server_url = \"http://hf-tgi.llm-hosting.svc.cluster.local:3000/\"\n",
    "redis_url = \"redis://server:port\"\n",
    "index_name = \"docs\"\n",
    "schema_name = \"redis_schema.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82970898",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e11d23-c0ad-4875-b67f-149fc8b14725",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores.redis import Redis\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import HuggingFaceTextGenInference\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4c1b1a",
   "metadata": {},
   "source": [
    "#### Initialize the connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb6a3e3-5ccd-441e-b80d-427555d9e9f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings()\n",
    "rds = Redis.from_existing_index(\n",
    "    embeddings,\n",
    "    redis_url=redis_url,\n",
    "    index_name=index_name,\n",
    "    schema=schema_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72a3a2b",
   "metadata": {},
   "source": [
    "#### Initialize query chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8fd396-0798-45c5-8969-6b6ede134c77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# NOTE: This template syntax is specific to Llama2\n",
    "template=\"\"\"<s>[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant.\n",
    "You will be given a question you need to answer, and a context to provide you with information. You must answer the question based as much as possible on this context.\n",
    "Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "<</SYS>>\n",
    "\n",
    "Question: {question}\n",
    "Context: {context} [/INST]\n",
    "\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
    "\n",
    "llm = HuggingFaceTextGenInference(\n",
    "    inference_server_url=inference_server_url,\n",
    "    max_new_tokens=512,\n",
    "    top_k=10,\n",
    "    top_p=0.95,\n",
    "    typical_p=0.95,\n",
    "    temperature=0.1,\n",
    "    repetition_penalty=1.175,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()]\n",
    ")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(llm,\n",
    "                                       retriever=rds.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4, \"distance_threshold\": 0.5}),\n",
    "                                       chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n",
    "                                       return_source_documents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a45ad23",
   "metadata": {},
   "source": [
    "#### Query example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105d2fd1-f36c-409d-8e52-ec6d23a56ad1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metadata key page not found in metadata. Setting to None. \n",
      "Metadata fields defined for this instance: ['source', 'page']\n",
      "Metadata key page not found in metadata. Setting to None. \n",
      "Metadata fields defined for this instance: ['source', 'page']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Hello! I'm here to assist you with your question about working with GPUs and taints in Kubernetes. Based on the provided context, I understand that you want to use taints to restrict access to GPUs and provide choice between different types of GPUs.\n",
      "\n",
      "To start, it's important to note that the NVIDIA Operator has a built-in tolerance for the nvidia.com/gpu taint, so you don't need to add it explicitly. However, if you want to schedule Pods on nodes with other types of GPUs, you'll need to apply the appropriate taints to those nodes.\n",
      "\n",
      "When applying taints, it's essential to pay close attention to avoid installing the NVIDIA drivers on non-NVIDIA GPU nodes. To do this, you can use the `tolerations` field in your Pod spec to specify which taints the Pod can tolerate. For example, if you want to schedule a Pod on a node with an NVIDIA GPU, you can set the `tolerations` field to include the nvidia.com/gpu taint.\n",
      "\n",
      "Here's an example of how you could apply taints to a Pod spec:\n",
      "```yaml\n",
      "apiVersion: apps/v1\n",
      "kind: Deployment\n",
      "spec:\n",
      "  selector:\n",
      "    matchLabels:\n",
      "      app: myapp\n",
      "  template:\n",
      "    metadata:\n",
      "      labels:\n",
      "        app: myapp\n",
      "    spec:\n",
      "      tolerations:\n",
      "      - key: nvidia.com/gpu\n",
      "        operator: Equal\n",
      "        value: True\n",
      "      containers:\n",
      "      - name: mycontainer\n",
      "        image: myimage\n",
      "        ports:\n",
      "        - containerPort: 80\n",
      "```\n",
      "In this example, the `tolerations` field specifies that the Pod can tolerate the nvidia.com/gpu taint. This means that the Pod will only be scheduled on nodes with NVIDIA GPUs.\n",
      "\n",
      "Keep in mind that you can apply multiple taints to a Pod spec to restrict access to specific types of GPUs. For example, if you want to schedule a Pod on a node with either an NVIDIA GTX 1080 Ti or an AMD Radeon VII, you could apply two taints: one for each type of GPU.\n",
      "```"
     ]
    }
   ],
   "source": [
    "question = \"How can I work with GPU and taints?\"\n",
    "result = qa_chain({\"query\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d75d0c",
   "metadata": {},
   "source": [
    "#### Retrieve source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acda357e-558a-4879-8ad8-21f0567f2f2e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://ai-on-openshift.io/odh-rhods/nvidia-gpus/\n"
     ]
    }
   ],
   "source": [
    "def remove_duplicates(input_list):\n",
    "    unique_list = []\n",
    "    for item in input_list:\n",
    "        if item.metadata['source'] not in unique_list:\n",
    "            unique_list.append(item.metadata['source'])\n",
    "    return unique_list\n",
    "\n",
    "results = remove_duplicates(result['source_documents'])\n",
    "\n",
    "for s in results:\n",
    "    print(s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
